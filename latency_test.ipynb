{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea2d40c78cff408ca8870791b1c1b0c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|eot_id|>\n",
      "128009\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from actqkv.models import LlamaForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from actqkv.utils import patch_hf, GreedySearch, patch_model_center\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "torch.cuda.set_device(0) \n",
    "\n",
    "conf = OmegaConf.load(\"/hpc2hdd/home/qxiao183/self-rag/Q-LLM/config/llama3-actqkv-repr4-init64-l512-bs32-topk46-w1-activate_dev_with_sigmod_topk_dynamic_dev.yaml\")\n",
    "model_path = \"/hpc2hdd/home/qxiao183/models/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    "    ).to(\"cuda:0\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, add_bos_token=True, add_eos_token=True)\n",
    "# 设置 pad_token 为 eos_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# 验证\n",
    "print(tokenizer.pad_token)     \n",
    "print(tokenizer.pad_token_id)   \n",
    "model = patch_hf(model, \"actqkv_dev\", conf.model)\n",
    "model = GreedySearch(model, tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefilling_latency: 8.2678s | Decoding_latency: 17.5097s | Total_latency: 25.8626s\n",
      "ActQKV Latency Test\n",
      "length of input 10401\n",
      "length of output 71\n",
      "['Repeat this is a latency test for prefilling and decoding.\\nRepeat this is a latency test for prefilling and decoding.\\nRepeat this is a latency test for prefilling and decoding.\\nRepeat this is a latency test for prefilling and decoding.\\nRepeat this is a latency test for prefilling and decoding.\\nRepeat this is a latency test for prefilling and decoding.\\nRepeat this is a latency test for prefilling and decoding.\\nRepeat this is a latency test for prefill']\n"
     ]
    }
   ],
   "source": [
    "text = \"Repeat this is a latency test for prefilling and decoding.\\n\" * 800\n",
    "\n",
    "tokenized_prompt = tokenizer(text, truncation=False, return_tensors=\"pt\", add_special_tokens=True).input_ids[0]\n",
    "\n",
    "\n",
    "output = model.generate(input_ids = tokenized_prompt, max_length=100)\n",
    "print(\"ActQKV Latency Test\")\n",
    "print(\"length of input\", len(tokenized_prompt))\n",
    "print(\"length of output\", len(output[0].split(\" \")))\n",
    "print(output)\n",
    "model.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ActQKV Latency Test\n",
      "Prefilling_latency: 113.4956s | Decoding_latency: 20.6463s | Total_latency: 134.2268s\n",
      "length of input 104001\n",
      "length of output 454\n",
      "['Repeat this is a latency test for prefilling and decoding.\\nRepeat this is a latency test for prefilling and decoding.\\nRepeat this is a latency test for prefilling and decoding.\\nRepeat this is a latency test for prefilling and decoding.\\nRepeat this is a latency test for prefilling and decoding.\\nRepeat this is a latency test for prefilling and decoding.\\nRepeat this is a latency test for prefilling and decoding.\\nRepeat this is a latency test for prefill']\n"
     ]
    }
   ],
   "source": [
    "text = \"Repeat this is a latency test for prefilling and decoding.\\n\" * 8000\n",
    "tokenized_prompt = tokenizer(text, truncation=False, return_tensors=\"pt\", add_special_tokens=True).input_ids[0]\n",
    "output = model.generate(input_ids = tokenized_prompt, max_length=100)\n",
    "print(\"ActQKV Latency Test\")\n",
    "print(\"length of input\", len(tokenized_prompt))\n",
    "print(\"length of output\", len(output[0].split(\" \")))\n",
    "print(output)\n",
    "model.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefilling_latency: 614.3588s | Decoding_latency: 25.2816s | Total_latency: 639.7242s\n",
      "ActQKV Latency Test\n",
      "length of input 520001\n",
      "length of output 454\n",
      "['Repeat this is a latency test for prefilling and decoding.\\nRepeat this is a latency test for prefilling and decoding.\\nRepeat this is a latency test for prefilling and decoding.\\nRepeat this is a latency test for prefilling and decoding.\\nRepeat this is a latency test for prefilling and decoding.\\nRepeat this is a latency test for prefilling and decoding.\\nRepeat this is a latency test for prefilling and decoding.\\nRepeat this is a latency test for prefill']\n"
     ]
    }
   ],
   "source": [
    "text = \"Repeat this is a latency test for prefilling and decoding.\\n\" * 40000\n",
    "tokenized_prompt = tokenizer(text, truncation=False, return_tensors=\"pt\", add_special_tokens=True).input_ids[0]\n",
    "output = model.generate(input_ids = tokenized_prompt, max_length=100)\n",
    "print(\"ActQKV Latency Test\")\n",
    "print(\"length of input\", len(tokenized_prompt))\n",
    "print(\"length of output\", len(output[0].split(\" \")))\n",
    "print(output)\n",
    "model.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9212d27f82e419ab86aa9721b5e76df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|eot_id|>\n",
      "128009\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from actqkv.models import LlamaForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from actqkv.utils import patch_hf, GreedySearch, patch_model_center\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "torch.cuda.set_device(0) \n",
    "\n",
    "conf = OmegaConf.load(\"/hpc2hdd/home/qxiao183/self-rag/Q-LLM/config/llama3-infllm-repr4-init64-l512-bs32-topk46-w1-latency.yaml\")\n",
    "model_path = \"/hpc2hdd/home/qxiao183/models/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    "    ).to(\"cuda:0\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, add_bos_token=True, add_eos_token=True)\n",
    "# 设置 pad_token 为 eos_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# 验证\n",
    "print(tokenizer.pad_token)     \n",
    "print(tokenizer.pad_token_id)   \n",
    "model = patch_hf(model, \"infllm\", conf.model)\n",
    "model = GreedySearch(model, tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefilling_latency: 7.7598s | Decoding_latency: 6.7624s | Total_latency: 14.6073s\n",
      "InfLLM Latency Test\n",
      "length of input 10401\n",
      "length of output 71\n",
      "['Repeat this is a latency test for prefilling and decoding.\\nRepeat this is a latency test for prefilling and decoding.\\nRepeat this is a latency test for prefilling and decoding.\\nRepeat this is a latency test for prefilling and decoding.\\nRepeat this is a latency test for prefilling and decoding.\\nRepeat this is a latency test for prefilling and decoding.\\nRepeat this is a latency test for prefilling and decoding.\\nRepeat this is a latency test for prefill']\n"
     ]
    }
   ],
   "source": [
    "text = \"Repeat this is a latency test for prefilling and decoding.\\n\" * 800\n",
    "\n",
    "tokenized_prompt = tokenizer(text, truncation=False, return_tensors=\"pt\", add_special_tokens=True).input_ids[0]\n",
    "\n",
    "\n",
    "output = model.generate(input_ids = tokenized_prompt, max_length=100)\n",
    "print(\"InfLLM Latency Test\")\n",
    "print(\"length of input\", len(tokenized_prompt))\n",
    "print(\"length of output\", len(output[0].split(\" \")))\n",
    "print(output)\n",
    "model.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefilling_latency: 61.7640s | Decoding_latency: 8.7453s | Total_latency: 70.5944s\n",
      "InfLLM Latency Test\n",
      "length of input 104001\n",
      "length of output 71\n",
      "['Repeat this is a latency test for prefilling and decoding.\\nRepeat this is a latency test for prefilling and decoding.\\nRepeat this is a latency test for prefilling and decoding.\\nRepeat this is a latency test for prefilling and decoding.\\nRepeat this is a latency test for prefilling and decoding.\\nRepeat this is a latency test for prefilling and decoding.\\nRepeat this is a latency test for prefilling and decoding.\\nRepeat this is a latency test for prefill']\n"
     ]
    }
   ],
   "source": [
    "text = \"Repeat this is a latency test for prefilling and decoding.\\n\" * 8000\n",
    "\n",
    "tokenized_prompt = tokenizer(text, truncation=False, return_tensors=\"pt\", add_special_tokens=True).input_ids[0]\n",
    "\n",
    "\n",
    "output = model.generate(input_ids = tokenized_prompt, max_length=100)\n",
    "print(\"InfLLM Latency Test\")\n",
    "print(\"length of input\", len(tokenized_prompt))\n",
    "print(\"length of output\", len(output[0].split(\" \")))\n",
    "print(output)\n",
    "model.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefilling_latency: 307.7571s | Decoding_latency: 9.8285s | Total_latency: 317.6694s\n",
      "InfLLM Latency Test\n",
      "length of input 520001\n",
      "length of output 71\n",
      "['Repeat this is a latency test for prefilling and decoding.\\nRepeat this is a latency test for prefilling and decoding.\\nRepeat this is a latency test for prefilling and decoding.\\nRepeat this is a latency test for prefilling and decoding.\\nRepeat this is a latency test for prefilling and decoding.\\nRepeat this is a latency test for prefilling and decoding.\\nRepeat this is a latency test for prefilling and decoding.\\nRepeat this is a latency test for prefill']\n"
     ]
    }
   ],
   "source": [
    "text = \"Repeat this is a latency test for prefilling and decoding.\\n\" * 40000\n",
    "\n",
    "tokenized_prompt = tokenizer(text, truncation=False, return_tensors=\"pt\", add_special_tokens=True).input_ids[0]\n",
    "\n",
    "\n",
    "output = model.generate(input_ids = tokenized_prompt, max_length=100)\n",
    "print(\"InfLLM Latency Test\")\n",
    "print(\"length of input\", len(tokenized_prompt))\n",
    "print(\"length of output\", len(output[0].split(\" \")))\n",
    "print(output)\n",
    "model.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
